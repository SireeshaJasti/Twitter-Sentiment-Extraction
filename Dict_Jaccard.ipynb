{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: regex in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied, skipping upgrade: click in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from nltk) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from nltk) (4.46.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: stanfordnlp in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied, skipping upgrade: protobuf in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from stanfordnlp) (3.12.3)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from stanfordnlp) (4.46.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from stanfordnlp) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.0.0 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from stanfordnlp) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from stanfordnlp) (2.24.0)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from protobuf->stanfordnlp) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from protobuf->stanfordnlp) (47.3.0.post20200616)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests->stanfordnlp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests->stanfordnlp) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests->stanfordnlp) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in c:\\users\\rangapotluri\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages (from requests->stanfordnlp) (2.9)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "import stanfordnlp\n",
    "\n",
    "import warnings\n",
    "def fxn():\n",
    "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    fxn()\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"en_ewt\" for language \"en\".\n",
      "Would you like to download the models for: en_ewt now? (Y/n)\n",
      "Y\n",
      "\n",
      "Default download directory: C:\\Users\\RangaPotluri\\stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "\n",
      "Downloading models for: en_ewt\n",
      "Download location: C:\\Users\\RangaPotluri\\stanfordnlp_resources\\en_ewt_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0.00/235M [00:09<?, ?B/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-897925946b72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstanfordnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stopwords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'punkt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'averaged_perceptron_tagger'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\stanfordnlp\\utils\\resources.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(download_label, resource_dir, confirm_if_exists, force, version)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Using the default treebank \"{default_treebanks[download_label]}\" for language \"{download_label}\".'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         download_ud_model(default_treebanks[download_label], resource_dir=resource_dir,\n\u001b[1;32m--> 137\u001b[1;33m                           confirm_if_exists=confirm_if_exists, force=force, version=version)\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The language or treebank \"{download_label}\" is not currently supported by this function. Please try again with other languages or treebanks.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\stanfordnlp\\utils\\resources.py\u001b[0m in \u001b[0;36mdownload_ud_model\u001b[1;34m(lang_name, resource_dir, should_unzip, confirm_if_exists, force, version)\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mdefault_chunk_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m67108864\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdefault_chunk_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 576\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m                 if (\n\u001b[0;32m    521\u001b[0m                     \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Unexpected EOF\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1837\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1838\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1839\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1840\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('en')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import tokenize\n",
    "\n",
    "# for word counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2-faced', '2-faces', 'abnormal', 'abolish', 'abominable', 'abominably', 'abominate', 'abomination', 'abort', 'aborted']\n"
     ]
    }
   ],
   "source": [
    "with open(\"negative-words.txt\", \"r\") as f:\n",
    "    neg_text = f.read() # neg_text is string\n",
    "neg_tokens = neg_text.split(\"\\n\")# taking all neg words into a list\n",
    "neg_tokens[-1:] = [] # removing last empty string\n",
    "print(neg_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a+', 'abound', 'abounds', 'abundance', 'abundant', 'accessable', 'accessible', 'acclaim', 'acclaimed', 'acclamation']\n"
     ]
    }
   ],
   "source": [
    "with open(\"positive-words.txt\", \"r\") as f:\n",
    "    pos_text = f.read() # neg_text is string\n",
    "pos_tokens = pos_text.split(\"\\n\")# taking all pos words into a list\n",
    "pos_tokens[-1:] = [] # removing last empty string\n",
    "print(pos_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27481, 4)\n",
      "(3534, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27481 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27481 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27481 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 858.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()# there are null values in the train data. we can drop those null observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>fdb77c3752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         textID text selected_text sentiment\n",
       "314  fdb77c3752  NaN           NaN   neutral"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27480 entries, 0 to 27480\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   textID         27480 non-null  object\n",
      " 1   text           27480 non-null  object\n",
      " 2   selected_text  27480 non-null  object\n",
      " 3   sentiment      27480 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3534 entries, 0 to 3533\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   textID     3534 non-null   object\n",
      " 1   text       3534 non-null   object\n",
      " 2   sentiment  3534 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 83.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info() # there are no null values in test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral     11117\n",
       "positive     8582\n",
       "negative     7781\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there are three class labels. let us check the frequency of the class labels\n",
    "temp = train.groupby('sentiment').count()['text'].sort_values(ascending = False)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1caac56a708>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# graphical distribution of class label in train set \n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(x='sentiment',data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1caac738f88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualization of class label in test set\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.countplot(x='sentiment',data=train)\n",
    "# if you observe in both train and test neutral is dominating both positive and negative\n",
    "# But the distribution of +ve and -ve is same in both train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are some special symbols in text and selected_text, these  are to be processed\n",
    "def clean_text(txt):\n",
    "    \n",
    "    txt = str(txt).lower() # Converting into lower\n",
    "    txt = re.sub('https?://\\S+|www\\.\\S+', '', txt)# removing links\n",
    "    txt = re.sub('<.*?>+', '', txt)# removing angular brackets\n",
    "    txt = re.sub('\\n', '', txt) # removing new lines \n",
    "    txt = re.sub('\\w*\\d\\w*', '', txt) # remove words containing numbers\n",
    "    txt = re.sub('[%s]' % re.escape(string.punctuation), '', txt)# remove punctuation\n",
    "    txt = re.sub('\\[.*?\\]', '', txt)# removing square brackets\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "train['text'] = train['text'].apply(lambda x:clean_text(x))\n",
    "train['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = pd.DataFrame()\n",
    "df_dict = train.filter(['text','sentiment','selected_text'], axis=1)\n",
    "df_dict['dict_word']= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27480\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing tweet words with dictionary words and extracting pos/neg words into new column 'dict_word'\n",
    "for i  in range(len(train)+1): \n",
    "    if(i == 314):\n",
    "        continue;\n",
    "    tweet = train['text'][i]\n",
    "    tweet_token = re.findall(r'\\b\\w[\\w-]*\\b', tweet.lower())\n",
    "    dict_words = []\n",
    "    flag = 0\n",
    "    for word in tweet_token:\n",
    "        if word in pos_tokens:\n",
    "            dict_words.append(word)\n",
    "            flag = 1\n",
    "                    \n",
    "        if word in neg_tokens:\n",
    "            dict_words.append(word)\n",
    "            flag = 1\n",
    "    if flag == 0:\n",
    "        df_dict['dict_word'][i] = tweet \n",
    "    else:\n",
    "        str1 = ' '.join(str(e) for e in dict_words)\n",
    "        df_dict['dict_word'][i] = str1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "1)Eventhough there are multiple pos/neg words always 'selected-text' is taking only the first pos/neg word. eg: tweet 6     \n",
    "2)Even the words are combined 'selected-text' is able to identify pos/neg words.eg: tweet 30    \n",
    "3)Eventhough the tweet has pos/neg word(s), in some cqases it is considering that as 'neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>dict_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad</td>\n",
       "      <td>sad miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>bullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons of</td>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>some shameless plugging for the best rangers...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>some shameless plugging for the best rangers...</td>\n",
       "      <td>shameless best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>feedings for the baby are fun when he is all ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun</td>\n",
       "      <td>fun smiles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soooo high</td>\n",
       "      <td>neutral</td>\n",
       "      <td>soooo high</td>\n",
       "      <td>soooo high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>both of you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>both of you</td>\n",
       "      <td>both of you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>journey wow u just became cooler  hehe is tha...</td>\n",
       "      <td>positive</td>\n",
       "      <td>wow u just became cooler</td>\n",
       "      <td>wow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as much as i love to be hopeful i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>as much as i love to be hopeful i reckon the c...</td>\n",
       "      <td>love hopeful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>i really really like the song love story by ta...</td>\n",
       "      <td>positive</td>\n",
       "      <td>like</td>\n",
       "      <td>like love swift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>my sharpie is running dangerously low on ink</td>\n",
       "      <td>negative</td>\n",
       "      <td>dangerously</td>\n",
       "      <td>my sharpie is running dangerously low on ink</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>i want to go to music tonight but i lost my voice</td>\n",
       "      <td>negative</td>\n",
       "      <td>lost</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test test from the lg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>test test from the lg</td>\n",
       "      <td>test test from the lg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>uh oh i am sunburned</td>\n",
       "      <td>negative</td>\n",
       "      <td>uh oh i am sunburned</td>\n",
       "      <td>uh oh i am sunburned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sok trying to plot alternatives as we speak sigh</td>\n",
       "      <td>negative</td>\n",
       "      <td>sigh</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ive been sick for the past few days  and thus ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sick</td>\n",
       "      <td>sick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>negative</td>\n",
       "      <td>onna</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>hes just not that into you</td>\n",
       "      <td>neutral</td>\n",
       "      <td>hes just not that into you</td>\n",
       "      <td>hes just not that into you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>oh marly im so sorry  i hope you find her soon</td>\n",
       "      <td>neutral</td>\n",
       "      <td>oh marly im so sorry  i hope you find her soon</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>playing ghost online is really interesting the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>interesting</td>\n",
       "      <td>interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>is cleaning the house for her family who is co...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>is cleaning the house for her family who is co...</td>\n",
       "      <td>is cleaning the house for her family who is co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gotta restart my computer  i thought  was supp...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gotta restart my computer  i thought  was supp...</td>\n",
       "      <td>gotta restart my computer  i thought  was supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>see wat i mean bout  friidays its called lose ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>see wat i mean bout  friidays its called lose ...</td>\n",
       "      <td>lose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the free fillin app on my ipod is fun im addicted</td>\n",
       "      <td>positive</td>\n",
       "      <td>the free fillin app on my ipod is fun im addicted</td>\n",
       "      <td>free fun addicted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>im sorry</td>\n",
       "      <td>negative</td>\n",
       "      <td>im sorry</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>on the way to malaysiano internet access to twit</td>\n",
       "      <td>negative</td>\n",
       "      <td>no internet</td>\n",
       "      <td>on the way to malaysiano internet access to twit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>juss came backk from berkeleyy  omg its madd f...</td>\n",
       "      <td>positive</td>\n",
       "      <td>fun</td>\n",
       "      <td>fun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>went to sleep and there is a power cut in noid...</td>\n",
       "      <td>negative</td>\n",
       "      <td>power back up not working too</td>\n",
       "      <td>went to sleep and there is a power cut in noid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>im going home now have you seen my new twitter...</td>\n",
       "      <td>positive</td>\n",
       "      <td>quiteheavenly</td>\n",
       "      <td>im going home now have you seen my new twitter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>i hope unni will make the audition  fighting d...</td>\n",
       "      <td>positive</td>\n",
       "      <td>hope</td>\n",
       "      <td>i hope unni will make the audition  fighting d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>if it is any consolation i got my bmi tested ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>well so much for being unhappy for about  minute</td>\n",
       "      <td>well unhappy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>thats very funny  cute kids</td>\n",
       "      <td>positive</td>\n",
       "      <td>funny</td>\n",
       "      <td>funny cute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ahhh i slept through the game  im gonna try m...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ahhh i slept through the game  im gonna try my...</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>thats it its the end tears for fears vs eric p...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>thats it its the end tears for fears</td>\n",
       "      <td>fears hero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>born and raised in nyc and living in texas for...</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>just in case you wonder we are really busy tod...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just in case you wonder we are really busy tod...</td>\n",
       "      <td>wonder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>im soooooo sleeeeepy the last day o school was...</td>\n",
       "      <td>negative</td>\n",
       "      <td>soooooo sleeeeepy</td>\n",
       "      <td>im soooooo sleeeeepy the last day o school was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>a little happy for the wine jeje ok itsm my fr...</td>\n",
       "      <td>positive</td>\n",
       "      <td>a little happy fo</td>\n",
       "      <td>happy free love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>car not happy big big dent in boot hoping the...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>car not happy big big dent in boot hoping they...</td>\n",
       "      <td>happy dent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>im an avid fan of  magazine and i love your ma...</td>\n",
       "      <td>positive</td>\n",
       "      <td>avid fan of</td>\n",
       "      <td>avid love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>mayday</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mayday</td>\n",
       "      <td>mayday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>ratt rocked nashville toniteone thing sucked n...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ratt rocked nashville toniteone thing sucked n...</td>\n",
       "      <td>sucked like fun bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>i love to but im only available from   and wh...</td>\n",
       "      <td>positive</td>\n",
       "      <td>i love to</td>\n",
       "      <td>love available love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "0                   id have responded if i were going   neutral   \n",
       "1          sooo sad i will miss you here in san diego  negative   \n",
       "2                              my boss is bullying me  negative   \n",
       "3                       what interview leave me alone  negative   \n",
       "4    sons of  why couldnt they put them on the rel...  negative   \n",
       "5     some shameless plugging for the best rangers...   neutral   \n",
       "6    feedings for the baby are fun when he is all ...  positive   \n",
       "7                                          soooo high   neutral   \n",
       "8                                         both of you   neutral   \n",
       "9    journey wow u just became cooler  hehe is tha...  positive   \n",
       "10   as much as i love to be hopeful i reckon the ...   neutral   \n",
       "11  i really really like the song love story by ta...  positive   \n",
       "12       my sharpie is running dangerously low on ink  negative   \n",
       "13  i want to go to music tonight but i lost my voice  negative   \n",
       "14                             test test from the lg    neutral   \n",
       "15                               uh oh i am sunburned  negative   \n",
       "16   sok trying to plot alternatives as we speak sigh  negative   \n",
       "17  ive been sick for the past few days  and thus ...  negative   \n",
       "18         is back home now      gonna miss every one  negative   \n",
       "19                         hes just not that into you   neutral   \n",
       "20   oh marly im so sorry  i hope you find her soon     neutral   \n",
       "21  playing ghost online is really interesting the...  positive   \n",
       "22  is cleaning the house for her family who is co...   neutral   \n",
       "23  gotta restart my computer  i thought  was supp...   neutral   \n",
       "24  see wat i mean bout  friidays its called lose ...   neutral   \n",
       "25  the free fillin app on my ipod is fun im addicted  positive   \n",
       "26                                           im sorry  negative   \n",
       "27   on the way to malaysiano internet access to twit  negative   \n",
       "28  juss came backk from berkeleyy  omg its madd f...  positive   \n",
       "29  went to sleep and there is a power cut in noid...  negative   \n",
       "30  im going home now have you seen my new twitter...  positive   \n",
       "31  i hope unni will make the audition  fighting d...  positive   \n",
       "32   if it is any consolation i got my bmi tested ...  negative   \n",
       "33                        thats very funny  cute kids  positive   \n",
       "34   ahhh i slept through the game  im gonna try m...   neutral   \n",
       "35  thats it its the end tears for fears vs eric p...   neutral   \n",
       "36  born and raised in nyc and living in texas for...  negative   \n",
       "37  just in case you wonder we are really busy tod...   neutral   \n",
       "38  im soooooo sleeeeepy the last day o school was...  negative   \n",
       "39  a little happy for the wine jeje ok itsm my fr...  positive   \n",
       "40   car not happy big big dent in boot hoping the...   neutral   \n",
       "41  im an avid fan of  magazine and i love your ma...  positive   \n",
       "42                                             mayday   neutral   \n",
       "43  ratt rocked nashville toniteone thing sucked n...   neutral   \n",
       "44   i love to but im only available from   and wh...  positive   \n",
       "\n",
       "                                        selected_text  \\\n",
       "0                   id have responded if i were going   \n",
       "1                                            sooo sad   \n",
       "2                                         bullying me   \n",
       "3                                      leave me alone   \n",
       "4                                            sons of    \n",
       "5     some shameless plugging for the best rangers...   \n",
       "6                                                 fun   \n",
       "7                                          soooo high   \n",
       "8                                         both of you   \n",
       "9                            wow u just became cooler   \n",
       "10  as much as i love to be hopeful i reckon the c...   \n",
       "11                                               like   \n",
       "12                                        dangerously   \n",
       "13                                               lost   \n",
       "14                             test test from the lg    \n",
       "15                               uh oh i am sunburned   \n",
       "16                                               sigh   \n",
       "17                                               sick   \n",
       "18                                               onna   \n",
       "19                         hes just not that into you   \n",
       "20   oh marly im so sorry  i hope you find her soon     \n",
       "21                                        interesting   \n",
       "22  is cleaning the house for her family who is co...   \n",
       "23  gotta restart my computer  i thought  was supp...   \n",
       "24  see wat i mean bout  friidays its called lose ...   \n",
       "25  the free fillin app on my ipod is fun im addicted   \n",
       "26                                           im sorry   \n",
       "27                                        no internet   \n",
       "28                                                fun   \n",
       "29                      power back up not working too   \n",
       "30                                      quiteheavenly   \n",
       "31                                               hope   \n",
       "32   well so much for being unhappy for about  minute   \n",
       "33                                              funny   \n",
       "34  ahhh i slept through the game  im gonna try my...   \n",
       "35               thats it its the end tears for fears   \n",
       "36                                               miss   \n",
       "37  just in case you wonder we are really busy tod...   \n",
       "38                                  soooooo sleeeeepy   \n",
       "39                                  a little happy fo   \n",
       "40  car not happy big big dent in boot hoping they...   \n",
       "41                                        avid fan of   \n",
       "42                                             mayday   \n",
       "43  ratt rocked nashville toniteone thing sucked n...   \n",
       "44                                          i love to   \n",
       "\n",
       "                                            dict_word  \n",
       "0                   id have responded if i were going  \n",
       "1                                            sad miss  \n",
       "2                                            bullying  \n",
       "3                       what interview leave me alone  \n",
       "4    sons of  why couldnt they put them on the rel...  \n",
       "5                                      shameless best  \n",
       "6                                          fun smiles  \n",
       "7                                          soooo high  \n",
       "8                                         both of you  \n",
       "9                                                 wow  \n",
       "10                                       love hopeful  \n",
       "11                                    like love swift  \n",
       "12       my sharpie is running dangerously low on ink  \n",
       "13                                               lost  \n",
       "14                             test test from the lg   \n",
       "15                               uh oh i am sunburned  \n",
       "16                                               plot  \n",
       "17                                               sick  \n",
       "18                                               miss  \n",
       "19                         hes just not that into you  \n",
       "20                                              sorry  \n",
       "21                                        interesting  \n",
       "22  is cleaning the house for her family who is co...  \n",
       "23  gotta restart my computer  i thought  was supp...  \n",
       "24                                               lose  \n",
       "25                                  free fun addicted  \n",
       "26                                              sorry  \n",
       "27   on the way to malaysiano internet access to twit  \n",
       "28                                                fun  \n",
       "29  went to sleep and there is a power cut in noid...  \n",
       "30  im going home now have you seen my new twitter...  \n",
       "31  i hope unni will make the audition  fighting d...  \n",
       "32                                       well unhappy  \n",
       "33                                         funny cute  \n",
       "34                                               best  \n",
       "35                                         fears hero  \n",
       "36                                               miss  \n",
       "37                                             wonder  \n",
       "38  im soooooo sleeeeepy the last day o school was...  \n",
       "39                                    happy free love  \n",
       "40                                         happy dent  \n",
       "41                                          avid love  \n",
       "42                                             mayday  \n",
       "43                                sucked like fun bad  \n",
       "44                                love available love  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.head(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['dict_word_dependent'] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(tweet, stop_words, nlp,dictword,sentiment):\n",
    "    \n",
    "    if(sentiment == 'neutral'):\n",
    "        return tweet\n",
    "\n",
    "    tweetList = nltk.sent_tokenize(tweet)# Splitting the text into sentences\n",
    "    \n",
    "    #dictword = df_dict['dict_word'][k]# most of the selected_text are taking the first pos/neg word\n",
    "    neg_pos_word = dictword[0]\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "    word_dep = []\n",
    "\n",
    "    for line in tweetList:\n",
    "\n",
    "        tweet_list = nltk.word_tokenize(line)\n",
    "        POSList = nltk.pos_tag(tweet_list) # Doing Part-of-Speech Tagging to each word\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        if(len(POSList) == 1):\n",
    "            newwordList.append(tweet)\n",
    "            \n",
    "        for i in range(0,len(POSList)-1):\n",
    "            if(POSList[i][1]==\"NN\" and POSList[i+1][1]==\"NN\"): # If two consecutive words are Nouns then they are joined together\n",
    "                newwordList.append(POSList[i][0]+POSList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(POSList[i][0])\n",
    "                if(i==len(POSList)-2):\n",
    "                    newwordList.append(POSList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList) \n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        POSList = nltk.pos_tag(wordsList)\n",
    "\n",
    "        doc = nlp(finaltxt) # Object of Stanford NLP Pipeleine\n",
    "        \n",
    "        # Getting the dependency relations betwwen the words\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "\n",
    "        # Coverting it into appropriate format\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                if(len(newwordList) < len(POSList)):\n",
    "                    dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 2)]\n",
    "                \n",
    "        for i in range(len(dep_node)):\n",
    "            for j in range(len(dep_node[0])):\n",
    "                if(neg_pos_word in dep_node[i][j]):\n",
    "                    word_dep = dep_node[i][0]+' '+dep_node[i][1]\n",
    "        \n",
    "    \n",
    "    return word_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\RangaPotluri\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                        | 0/27480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading processors!\n",
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 27480/27480 [39:53<00:00, 11.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "nlp = stanfordnlp.Pipeline()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for i in tqdm(range(len(df_dict))):\n",
    "    if(i == 314):\n",
    "        continue;\n",
    "    tweet = df_dict['text'][i]\n",
    "    dictword = df_dict['dict_word'][i]\n",
    "    sentiment = df_dict['sentiment'][i]\n",
    "    dict_word_dep = sentiment_analysis(tweet, stop_words, nlp,dictword,sentiment)\n",
    "    if(dict_word_dep == []):\n",
    "        df_dict['dict_word_dependent'][i] = tweet\n",
    "    else:\n",
    "        df_dict['dict_word_dependent'][i] = dict_word_dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['dict_word_dependent'] = df_dict['dict_word_dependent'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>dict_word</th>\n",
       "      <th>dict_word_dependent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>neutral</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "      <td>id have responded if i were going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sooo sad i will miss you here in san diego</td>\n",
       "      <td>negative</td>\n",
       "      <td>sooo sad</td>\n",
       "      <td>sad miss</td>\n",
       "      <td>san</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me</td>\n",
       "      <td>negative</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>bullying</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>negative</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "      <td>what interview leave me alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sons of</td>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "      <td>sons of  why couldnt they put them on the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>happy mothers day to all you mums out there</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy mothers day to all you mums out there</td>\n",
       "      <td>happy</td>\n",
       "      <td>there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>caseys gone but why so she piddled a little o...</td>\n",
       "      <td>negative</td>\n",
       "      <td>freaked</td>\n",
       "      <td>caseys gone but why so she piddled a little o...</td>\n",
       "      <td>caseys gone but why so she piddled a little o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>hemp cloth is marvelous but unfortunately no</td>\n",
       "      <td>negative</td>\n",
       "      <td>unfortunately</td>\n",
       "      <td>marvelous unfortunately</td>\n",
       "      <td>marvelous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>gonna read a story bout adam lambert online th...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gonna read a story bout adam lambert online th...</td>\n",
       "      <td>gonna read a story bout adam lambert online th...</td>\n",
       "      <td>gonna read a story bout adam lambert online th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>we saw that in none   the baddies the best</td>\n",
       "      <td>positive</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "0                   id have responded if i were going   neutral   \n",
       "1          sooo sad i will miss you here in san diego  negative   \n",
       "2                              my boss is bullying me  negative   \n",
       "3                       what interview leave me alone  negative   \n",
       "4    sons of  why couldnt they put them on the rel...  negative   \n",
       "..                                                ...       ...   \n",
       "95        happy mothers day to all you mums out there  positive   \n",
       "96   caseys gone but why so she piddled a little o...  negative   \n",
       "97       hemp cloth is marvelous but unfortunately no  negative   \n",
       "98  gonna read a story bout adam lambert online th...   neutral   \n",
       "99         we saw that in none   the baddies the best  positive   \n",
       "\n",
       "                                        selected_text  \\\n",
       "0                   id have responded if i were going   \n",
       "1                                            sooo sad   \n",
       "2                                         bullying me   \n",
       "3                                      leave me alone   \n",
       "4                                            sons of    \n",
       "..                                                ...   \n",
       "95        happy mothers day to all you mums out there   \n",
       "96                                            freaked   \n",
       "97                                      unfortunately   \n",
       "98  gonna read a story bout adam lambert online th...   \n",
       "99                                               best   \n",
       "\n",
       "                                            dict_word  \\\n",
       "0                   id have responded if i were going   \n",
       "1                                            sad miss   \n",
       "2                                            bullying   \n",
       "3                       what interview leave me alone   \n",
       "4    sons of  why couldnt they put them on the rel...   \n",
       "..                                                ...   \n",
       "95                                              happy   \n",
       "96   caseys gone but why so she piddled a little o...   \n",
       "97                            marvelous unfortunately   \n",
       "98  gonna read a story bout adam lambert online th...   \n",
       "99                                               best   \n",
       "\n",
       "                                  dict_word_dependent  \n",
       "0                   id have responded if i were going  \n",
       "1                                                san   \n",
       "2                                                 me   \n",
       "3                       what interview leave me alone  \n",
       "4    sons of  why couldnt they put them on the rel...  \n",
       "..                                                ...  \n",
       "95                                             there   \n",
       "96   caseys gone but why so she piddled a little o...  \n",
       "97                                         marvelous   \n",
       "98  gonna read a story bout adam lambert online th...  \n",
       "99                                              best   \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    if (len(a) == 0):\n",
    "        return 0\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccard score is: 0.5105726804219353\n"
     ]
    }
   ],
   "source": [
    "df_dict['jaccard'] = df_dict.apply(lambda x: jaccard(x['selected_text'], x['dict_word_dependent']), axis = 1)\n",
    "\n",
    "print('The jaccard score is:', np.mean(df_dict['jaccard']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
